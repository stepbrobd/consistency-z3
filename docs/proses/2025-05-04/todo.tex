\section{Todo}

% failure/counter example
% 3 failure test cases in composition
% 21 failure test cases pairwise
% add back fafilure senario in XC

% principal of how to model apps (when to use symbol, when to use instantiation)
When modeling systems for consistency analysis, choosing the right
approach to abstract away underlying system behavior is crucial.
We propose the following principles to guide the modeling process:

% using default symbols
\textbf{Default symbols} are appropriate when system has straightforward
composition patterns with minimal cross-service dependencies.
Services should have well-defined consistency guarantees that don't
require fine-grained differentiation.
The primary concern is with verifying that the overall system
composition satisfies basic consistency requirements.

As shown in the online shopping mall example, default symbols work well when
modeling a system where each component (shop, transaction log,
arbitrator) has clear, isolated responsibilities. The focus is on
ensuring that the composition of these components maintains the
desired consistency guarantees without needing to track specific data
objects or operation instances.

% operation instantiation
\textbf{Operation instantiations} should be used
when the system's consistency depends on the ordering of specific API
calls or RPC functions.
This is most useful when there are ordering constraints on API/RPC
calls or dependencies between actions (as in flow of data) across services
The focus is on the behavior of the system rather than the underlying
data storage.

The media example illustrates this approach by modeling each API
action as a distinct operation instance. This allows specification of
ordering constraints between operations,
such as requiring login before accessing content or checking metadata
before serving videos.

% custom symbols
\textbf{Custom symbols} become necessary when different storage
systems maintain separate state spaces that need to
be distinguished between each other, such as cross service ordering
constraints exist between specific data objects.
Or we need to track how consistency properties apply to particular data
elements across system boundaries. This means that each consistency model
object instantiation will be different even though the same model is used.
This is required as the system's internal behavior are captured using one set
of symbolic representations, while the external behavior is captured using
another. When interactions are required between internal and external, a shim
layer is needed to translate between the two representations (i.e.
symbolic equivalence in cross-causal).

The cross-causal (XC) example demonstrates this approach, where each
storage system has its own state and the consistency semantics must
be explicitly differentiated using custom symbols.

\section{Related Work}

% FIXME: rework
% add smt for coverage checking paper
{\color{red}

  \section{Toward Domain-Specific Solvers for Distributed Consistency}

  \subsection{Summary}

  In Kuper et al.~\cite{kuper2019toward}, the authors argued
  embedding automated theorem
  solvers into a language like
  LiquidHaskell~\cite{vazou2014liquidhaskell} and Rosette
  \cite{torlak2014a}, and use these solver-aided languages implement
  distributed systems
  can result in consistent-by-construction applications. However, transitive
  relations, partial orderings, and external API invocations are difficult
  properties to reason in these solver-aided languages due to non-trivial
  encodings and efficiency concerns. The authors advocates for developing
  domain-specific SMT solvers that are specialized for reasoning
  about distributed
  consistency properties, and programmers should not need to fully understand
  satisfiability modulo theories in order to implement theory solvers for their
  domain of interest with the help of these domain-specific solver frameworks.

  \subsection{Methodology}

  The paper presents a vision rather than a specific implementation, drawing on
  experiences with existing systems to motivate the need for domain-specific
  solvers. The methodology focuses on identifying key challenges and outlining a
  research agenda rather than providing concrete solutions.

  The authors listed three key features a "consistency-aware" solver
  should have:

  \begin{itemize}
    \item CRDT verifications: current verification of CRDT properties
      in Liquid Haskell
      requires non-trivial encodings and takes up to 40 seconds for
      basic ordering
      laws.

    \item Message reorderings: for lineage-driven fault injection
      (LDFI), the ability to
      efficiently reason about message commutativity could enable
      exploration of bugs
      triggered by message races while avoiding state explosion.
      Personally, this does
      not seem to be a target area for our work.

    \item Transitive closures: in
      Quelea~\cite{sivaramakrishnan2015declarative}, the inability
      to precisely express transitive closure in first-order logic forces overly
      conservative approximations of happens-before relationships.
      While this is a
      very valid, and on-point concern, however, transitive closures
      are encodable in
      solver frameworks with quantifiers elimination tactics and
      recursive definitions
      (which are standards in
      SMT-LIB\footnote{https://smt-lib.org/papers/smt-lib-reference-v2.6-r2017-07-18.pdf}).
  \end{itemize}

  \subsection{Relavance}

  On a first glance, the paper seem to reject our thesis where we did used
  general-purpose SMT solvers like Z3~\cite{demoura2008z3} to reason
  about distributed
  consistency properties. Throughout the development of our framework, we have
  encountered the same issues as the authors mentioned, where the encoding of
  transitive relations, partial orderings, and non-deterministic operations are
  non-trivial and inefficient.

  However, rather than rejecting general-purpose solvers entirely, our work
  demonstrates that careful encoding strategies and abstraction choices can help
  mitigate some of these limitations. We take inspiration from the
  paper's vision
  of domain-specific solvers while working within the constraints of existing
  tools.

  Instead of trying to deduce a global ordering of events from a set of axioms
  (not decidable in the first place), we avoid explicit reasoning about state
  machine transitions (i.e. explicit version vectors or Lamport clocks), our
  framework treats consistency guarantees as logical constraints over abstract
  executions. The checking call to solvers reduces verification to constraint
  satisfaction problems (negation of implication) that are decidable in
  first-order logic. However, we acknowledge that domain-specific solvers as
  proposed could potentially make our approach even more efficient.

  \section{Satisfiability Modulo Ordering Consistency Theory for SC,
  TSO, and PSO Memory Models}

  \subsection{Summary}

  Fan et al.~\cite{fan2023satisfiability} propose a ordering
  consistency theory for
  verifying multi-threaded programs under sequential consistency
  (SC), TSO and PSO
  (total/partial store order) memory models. While partial orders can concisely
  represent thread interleavings, existing approaches using integer difference
  logic to encode ordering constraints are inefficient as they: compute exact
  clock values when only relative ordering matters, and eagerly encode all
  possible from-read orders (read-from and from-read are extramely
    similar to our
  visible/visibility definition) regardless of whether they are needed. The
  authors develop a theory solver that performs incremental
  consistency checking,
  generates minimal conflict clauses, and includes specialized theory
  propagation.

  \subsection{Methodology}

  The paper formalizes their ordering consistency theory
  $\mathcal{T}_{ord}$ with
  signature where (Section 4.1):

  $$
  \Sigma_{ord} = \{ e_1, e_2, ..., \prec_{ppo}, \prec_{ws},
  \prec_{rf}, \prec_{fr}, \approx \}
  $$

  \begin{itemize}
    \item $e_i$ are memory access events in $EE$
    \item $\prec_{ppo}$ represents preserved program order (binary predicate)
    \item $\prec_{ws}$ represents write serialization (binary predicate, total
      order of writes to same address)
    \item $\prec_{rf}$ represents read-from relation (binary predicate,
      linking writes to reads)
    \item $\prec_{fr}$ represents from-read relation (binary predicate,
      derived from $\prec_{ws}$ and $\prec_{rf}$)
    \item $\approx$ represents atomicity equivalence relation
  \end{itemize}

  Where the 4 axioms of $\mathcal{T}_{ord}$ are:

  \begin{itemize}
    \item partial order where $\prec_{ws}$ only relates write events,
      $\prec_{rf}$ link
      write events to read events, and $\prec_{fr}$ link read events to
      write events
    \item equivalence relation (trivial reflexivity)
    \item from-read derivation where it's similar to our monotonic
      read definition
      (visibility included)
    \item consistency where $\prec_{ws} \cup \prec_{rf} \cup \prec_{fr}$ is
      consistent with $\prec_{ppo}$ and $\approx$
  \end{itemize}

  The solver maintains an event graph (think of this as abstract
  executions) where
  nodes are events and edges represent ordering relations. Rather than eagerly
  encoding all possible from-read orders, it derives them on-demand
  during solving
  when relevant variables are assigned. Consistency checking reduces to cycle
  detection in this graph.

  The solver performs incremental cycle detection by maintaining a
  pseudo-topological order of events and updating it efficiently when edges are
  added. When inconsistency is detected, it generates minimal
  conflict clauses by
  finding critical cycles with the smallest number of induced edges. This
  formalization aligns closely with our visibility/visible definitions (and
  session semantic definitions), but they treat read-from and from-read as
  separate relations rather than doing explicit case analysis and combining them
  into a single visibility relation.

  \subsection{Relavance}

  The paper's formalization approach provides important insights for our work.
  Their rigorous axiomatization of ordering relations provides a
  template for how
  we could formalize our consistency models more precisely (instead of using
  proses like how we have).

  While we focused on higher-level consistency specifications rather than
  low-level memory model semantics, and considering we combined read-from and
  from-read into a single visibility relation with case analysis rather than
  treating them separately, this paper demonstrates the benefits of having a
  precise formal theory with clear axioms and efficient decision
  procedures. Even
  though the domain is different, many of their theoretical techniques could
  potentially be adapted to make our framework more rigorous and efficient.

  Couple points to note: we should take their pseudocode example and layout our
  checking logic. They also made Fig. 3 to demonstrate the differences of event
  graphs constructed from different relations, we should also consider this to
  show the case analysis for visibility definition For application
  level checking,
  Fig. 5 and Fig. 6 should be taken into consideration, and we should implement
  SMT-LIB exporter to make our tool checker agnostic. The paper also have
  experiments on evaluations, we should also consider this.

}
